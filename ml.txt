Random Forests + Gradient Boosting!

-1  Overfit your data!
 0  Shuffle + split data: Train, Dev/CV, Test; stratify as needed
 1  Normalize data as needed
 2  Simple algorithm: quick rough draft
 3  Sanity check: small training sample
 4  Learning curves: plot error vs iteration/model/lambda/training size
 5  Error analysis: F score/log Loss, manual inspection
 6  Identify bias (model) or variance (learning) issue
 7  Ceiling analysis: resource allocation

Logistic Regression/Linear SVM: Not enough data (relative to no. features)
Kernel SVM: "medium" data size; too slow for large data
PCA: retain % variance; faster learning, data compression, visualization

Minibatch Gradient Descent: batch of eg 10 (range 2-100)
Convergence: decrease learning rate over time

Data Synthesis: from scratch/combined piecemeal, adding distortion/noise
Size Matters: How much work would it take to get 10x current data?
Ceiling Analysis: identify best case/upside/weak links in pipeline

Leakage: of info/data via train/test data, features, etc

---

Supervised Learning:
linear regression
logistic regression
neural networks
SVM

Unsupervised Learning:
K-means
PCA
anomaly detection

---

"You must earn your complexity"

Feature Transform: PCA, RCA, ICA

Restriction bias: what the model is able to represent
Preference bias: given 2 representations, which the model prefers

Boosting: weighted ensemble of weak learners to form strong learner
    avoids overfitting(?)
    works everywhere

Optimization: MIMIC, random restart, annealing, genetic